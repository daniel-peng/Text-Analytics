{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjor/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import operator\n",
    "import re,string\n",
    "from patsy import dmatrices\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. What are the top 5 parts of speech in this corpus of job descriptions? How frequently do they appear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/pranjor/Documents/UT/MSBA/Text Analytics/project 1/Train_rev1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "description  = train['FullDescription'][:1000]\n",
    "\n",
    "corpus = description.str.cat(sep=' ')\n",
    "\n",
    "corpus = corpus.decode('utf-8')\n",
    "\n",
    "corpus_words = nltk.word_tokenize(corpus.lower())\n",
    "corpus_words = [word for word in corpus_words if word.isalpha()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Calculate the frequencies of PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'engineering', 'NN'),\n",
       " (u'systems', 'NNS'),\n",
       " (u'analyst', 'NN'),\n",
       " (u'dorking', 'VBG'),\n",
       " (u'surrey', 'JJ'),\n",
       " (u'salary', 'JJ'),\n",
       " (u'our', 'PRP$'),\n",
       " (u'client', 'NN'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'located', 'VBN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = nltk.pos_tag(corpus_words)\n",
    "\n",
    "pos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjor/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>51790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>23246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>23123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>17170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>15271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word\n",
       "pos       \n",
       "NN   51790\n",
       "JJ   23246\n",
       "IN   23123\n",
       "DT   17170\n",
       "NNS  15271"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_freq = sorted(pos, key=lambda x: x[1],reverse=True)\n",
    "\n",
    "df_pos = pd.DataFrame(pos_freq)\n",
    "\n",
    "\n",
    "df_pos.columns = ['word','pos']\n",
    "\n",
    "df_pos.groupby('pos').count().sort('word',ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Does this corpus support Zipfâ€™s law? Plot the most common 100 words in the corpus against the theoretical prediction of the law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Calculate the frequencies of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>8435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>7121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>6677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>5812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>4547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>4229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>for</td>\n",
       "      <td>3450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>care</td>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with</td>\n",
       "      <td>2652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>you</td>\n",
       "      <td>2469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  freq\n",
       "0   and  8435\n",
       "1   the  7121\n",
       "2    to  6677\n",
       "3     a  5812\n",
       "4    of  4547\n",
       "5    in  4229\n",
       "6   for  3450\n",
       "7  care  3033\n",
       "8  with  2652\n",
       "9   you  2469"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_sorted = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "df = pd.DataFrame.from_records(c_sorted)\n",
    "df.columns = ['word','freq']\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Get rid of the punctuation marks and then rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = ~df['word'].isin(set(string.punctuation)) \n",
    "\n",
    "df = df[mask]\n",
    "\n",
    "df['rank'] = df['freq'].rank(method='min',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>8435</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>7121</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>6677</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>5812</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>4547</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  freq  rank\n",
       "0  and  8435   1.0\n",
       "1  the  7121   2.0\n",
       "2   to  6677   3.0\n",
       "3    a  5812   4.0\n",
       "4   of  4547   5.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5][['word','freq','rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Plot the most common 100 words in the corpus against the theoretical prediction of the law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ed23950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['x^-1']= [c**-1 for c in df['rank']]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = plt.plot([math.log(c) for c in df['rank'].values], [math.log(c) for c in df['freq']], 'ro',color='black')\n",
    "\n",
    "ax2 = plt.plot([math.log(c) for c in df['rank'].values], [math.log(c) for c in df['x^-1']], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. If we remove stopwords and lemmatize the corpus, what are the 10 most common words? What is their frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'care', 3060),\n",
       " (u'home', 2117),\n",
       " (u'experience', 1366),\n",
       " (u'manager', 1276),\n",
       " (u'nurse', 1266),\n",
       " (u'work', 1201),\n",
       " (u'nursing', 1180),\n",
       " (u'support', 1093),\n",
       " (u'within', 1040),\n",
       " (u'working', 965)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "corpus_words= [word for word in corpus_words if word not in set(string.punctuation)]\n",
    "\n",
    "filtered_words_sw = [word for word in corpus_words if word not in stopwords.words('english')]\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "words_lem = [wnl.lemmatize(word) for word in filtered_words_sw]\n",
    "\n",
    "fdist_2 = nltk.FreqDist(words_lem)\n",
    "fdist_2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## B1 Create a classification model with all words and the bag-of-words approach. How accurate is the model (show the confusion matrix)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a function that counts the frequency of each token in a document\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in corpus_words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find the cutoff salary\n",
    "cutoff = np.percentile(train[['SalaryNormalized']], 75)\n",
    "train['category'] = 'high'\n",
    "train.ix[train.SalaryNormalized < cutoff, 'category'] = 'low'\n",
    "\n",
    "train['token'] = \"\"\n",
    "fullset = train[['FullDescription','token','category']][:1000]\n",
    "\n",
    "#calculate the document_features in each document\n",
    "from __future__ import unicode_literals\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    token = nltk.word_tokenize(words.lower().decode('utf-8'))\n",
    "    feature = document_features(token)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is  0.822\n"
     ]
    }
   ],
   "source": [
    "print 'the accuracy is ', nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |             h |\n",
      "     |      l      i |\n",
      "     |      o      g |\n",
      "     |      w      h |\n",
      "-----+---------------+\n",
      " low | <80.6%>  8.6% |\n",
      "high |   9.2%  <1.6%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "test_mod = [x[0] for x in test_set]\n",
    "predict = classifier.classify_many(test_mod)\n",
    "actual = [x[1] for x in test_set]\n",
    "cm = nltk.ConfusionMatrix(actual, predict)\n",
    "print (cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2 . Speculate before running the following analysis whether lemmatization would help improve the accuracy of classification. Now create a classification model after lemmatization. Did the classification accuracy increase relative to B1? Comment on your speculation versus the actual results you obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmtization should improve the accuracy since there will be fewer tokens. With each token appearing in a document, the likelihood of the document being positive or negative will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatize words according to their pos in the corpus\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    token = nltk.word_tokenize(words.lower().decode('utf-8'))\n",
    "    pos_i = nltk.pos_tag(token)\n",
    "    wordnet_tag ={'NN':'n','JJ':'a','VB':'v','RB':'r'}\n",
    "    resultList = []\n",
    "    for t in pos_i:\n",
    "        try: resultList.append(wnl.lemmatize(t[0],wordnet_tag[t[1][:2]]))\n",
    "        except: resultList.append(wnl.lemmatize(t[0]))\n",
    "    feature = document_features(resultList)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run classification model\n",
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy:  0.82\n"
     ]
    }
   ],
   "source": [
    "print 'the accuracy: ', nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |             h |\n",
      "     |      l      i |\n",
      "     |      o      g |\n",
      "     |      w      h |\n",
      "-----+---------------+\n",
      " low | <80.8%>  8.4% |\n",
      "high |   9.6%  <1.2%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "test_mod = [x[0] for x in test_set]\n",
    "predict = classifier.classify_many(test_mod)\n",
    "actual = [x[1] for x in test_set]\n",
    "cm = nltk.ConfusionMatrix(actual, predict)\n",
    "print (cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After lemmatization, the accuracy actually went down from 0.822 to 0.82. Hence lemmatization has minimal impact on the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Now speculate whether stopwords removal from the original data would help increase the accuracy of the model. Take out the stopwords (but do not lemmatize), build a classification model and check the accuracy, and compare with that in B1 & B2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words were did not have strong predictive power in our classification model. It should not change the accuracy significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    words = re.sub(\"[^a-zA-Z]\", \" \", words)\n",
    "    words = words.lower().split()\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    clean_data = \" \".join(meaningful_words)\n",
    "    token = nltk.word_tokenize(clean_data.lower().decode('utf-8'))\n",
    "    feature = document_features(token)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    }
   ],
   "source": [
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "      contains(regional) = True             high : low    =     20.9 : 1.0\n",
      "  contains(relationship) = True             high : low    =     20.9 : 1.0\n",
      " contains(profitability) = True             high : low    =     20.9 : 1.0\n",
      " contains(presentations) = True             high : low    =     18.1 : 1.0\n",
      "    contains(governance) = True             high : low    =     18.1 : 1.0\n",
      "        contains(doctor) = True             high : low    =     16.3 : 1.0\n",
      "contains(decommissioning) = True             high : low    =     16.3 : 1.0\n",
      "          contains(aero) = True             high : low    =     16.3 : 1.0\n",
      "         contains(units) = True             high : low    =     16.3 : 1.0\n",
      "     contains(inservice) = True             high : low    =     16.3 : 1.0\n",
      "None\n",
      "     |             h |\n",
      "     |      l      i |\n",
      "     |      o      g |\n",
      "     |      w      h |\n",
      "-----+---------------+\n",
      " low | <80.6%>  8.6% |\n",
      "high |   9.4%  <1.4%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classifier.show_most_informative_features(10)\n",
    "\n",
    "# Printing confusion matrix\n",
    "test_set_mod = [x[0] for x in test_set]\n",
    "\n",
    "predicted_categories_NaiveBayes = classifier.classify_many(test_set_mod)\n",
    "actual_categories = [x[1] for x in test_set]\n",
    "\n",
    "cm = nltk.ConfusionMatrix(actual_categories, predicted_categories_NaiveBayes)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4 .  Use the job descriptions without lemmatiztion and stopword removal. Add parts-of-speech bigrams to the bag-of-words, and run a new classification model. Does the accuracy increase over the results in B1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Add parts-of-speech bigrams to the bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_bigram = list(nltk.bigrams(pos))\n",
    "pattern = [('JJ', 'NN'),('JJ','NNS'),('NN','NNS')]\n",
    "import copy\n",
    "corpus_words_bi = copy.deepcopy(corpus_words)\n",
    "\n",
    "for i in range(len(pos_bigram)):\n",
    "    (a,b),(c,d) = pos_bigram[i]\n",
    "    if (b,d) in pattern:\n",
    "        corpus_words_bi.append(' '.join(corpus_words[i:i+len(pattern)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Define the function to create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features_bigram(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for wd in corpus_words_bi:\n",
    "        features['contains({})'.format(wd)] = (wd in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Find out the unigrams and bigrams in each job description that match the part-of-speech patterns we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    tokens = nltk.word_tokenize(words.lower().decode('utf-8'))\n",
    "    tokens = [w for w in tokens if w.isalpha()==True]\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    bigram = list(nltk.bigrams(tags))\n",
    "    for n in range(len(bigram)):\n",
    "        (a,b),(c,d) = bigram[n]\n",
    "        if (b,d) in pattern:\n",
    "            tokens.append(' '.join(tokens[n:n+len(pattern)]))\n",
    "    feature = document_features_bigram(tokens)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Run classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.804"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |             h |\n",
      "     |      l      i |\n",
      "     |      o      g |\n",
      "     |      w      h |\n",
      "-----+---------------+\n",
      " low | <78.4%> 10.8% |\n",
      "high |   8.8%  <2.0%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "test_mod = [x[0] for x in test_set]\n",
    "predict = classifier.classify_many(test_mod)\n",
    "actual = [x[1] for x in test_set]\n",
    "cm = nltk.ConfusionMatrix(actual, predict)\n",
    "print (cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
